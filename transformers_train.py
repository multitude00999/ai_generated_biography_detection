# -*- coding: utf-8 -*-
"""transformers_classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1liJjU7A_6wevBOCR5Kt4K2BQfWfoFdU8

## Change to project directory and wandb login
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/DeepLearning_HW1

# # # Commented out IPython magic to ensure Python compatibility.
# # ! git clone https://github.com/vTheWise/MSAI437_Deep_Learning_HW1.git
# %cd MSAI437_Deep_Learning_HW1/

# !pip install transformers datasets evaluate
# !pip install wandb

import csv
import wandb
from datasets import load_dataset
import pandas as pd
from datasets import load_dataset, Features
from transformers import AutoTokenizer
from transformers import BertConfig
from transformers import AutoModelForSequenceClassification
from datasets import load_metric
import numpy as np
from transformers import TrainingArguments
from transformers import Trainer
import evaluate
import argparse
import csv
from matplotlib import pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report



"""## Merge files"""

def preprocess_line(line):
  line = line.replace('\n', '')
  line = line.strip()

  return line

def read_file(filename, label):
  file_content = open(filename, 'r').readlines()
  # print(file_content)
  bio = []
  temp_bio = []
  i = 0
  while i < len(file_content):
    line = preprocess_line(file_content[i])
    if line == '':
      i+=1
      continue
    elif line == '<end_bio>':
      
      if label == None:
        temp_bio.append(line)
        while i <len(file_content) and line!= '[REAL]' and line!= '[FAKE]':
          line = preprocess_line(file_content[i])
          i+=1

        # temp_bio.append(line)
        temp_bio_str = ' '.join(temp_bio)
        bio.append([temp_bio_str, line])
        temp_bio = []
      else:
        temp_bio.append(line)
        temp_bio_str = ' '.join(temp_bio)
        bio.append([temp_bio_str, label])
        temp_bio = []
      # break
    else:
      temp_bio.append(line)

    i+=1
    
  return bio

def merge_file_data(filename_list, label_list):
  data = []
  for (filename, label) in zip(filename_list, label_list):
    data = data + read_file(filename, label)
  return data


def add_labels_to_data():
  train_data = merge_file_data(['real.train.txt','fake.train.txt', 'mix.train.txt'],['[REAL]','[FAKE]', None])
  val_data = merge_file_data(['real.valid.txt','fake.valid.txt', 'mix.valid.txt'],['[REAL]','[FAKE]', None])
  test_data = merge_file_data(['real.test.txt','fake.test.txt', 'mix.test.txt'],['[REAL]','[FAKE]', None])

  df_training = pd.DataFrame(train_data,columns=["bio",'label'])

  df_valid = pd.DataFrame(val_data,columns=["bio",'label'])

  df_test = pd.DataFrame(test_data,columns=["bio",'label'])

  print("with duplicates:", " train", len(df_training), "val", len(df_valid), "test", len(df_test))

  df_training.drop_duplicates(inplace=True)
  df_valid.drop_duplicates(inplace=True)
  df_test.drop_duplicates(inplace=True)

  print("without duplicates", " train", len(df_training), "val", len(df_valid), "test", len(df_test))

  df_training.to_csv('./data/combined_train.txt', index=None, sep='\t')
  df_valid.to_csv('./data/combined_valid.txt', index=None, sep='\t')
  df_test.to_csv('./data/combined_test.txt', index=None, sep='\t')



"""## Dataloader"""

def tokenize_data(example, tokenizer):
  return tokenizer(example['bio'], padding='max_length', truncation = True)

def transform_labels(example):
  label = example['label']
  if label == '[REAL]':
    num = 0
  
  else:
    num = 1

  return {'labels': num}


def get_dataloader(tokenizer):
  dataset = load_dataset('csv',\
                         data_files={'train': './data/combined_train.txt', 'val': './data/combined_valid.txt', \
                                     'test': './data/combined_test.txt'}, delimiter = '\t')

  dataset = dataset.map(lambda k: tokenize_data(k, tokenizer), batched=True)

  dataset = dataset.map(transform_labels, remove_columns = ['bio', 'label'])

  return dataset


def compute_metrics(eval_pred):
  acc_metric = load_metric('accuracy')
  predictions, labels = eval_pred
  predictions = np.argmax(predictions, axis=1)
  return acc_metric.compute(predictions=predictions, references=labels)

"""## Training"""

def train(model, tokenizer, dataset):

  model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased",  num_labels=2)
  train_dataset = dataset['train'].shuffle(seed=2020)
  eval_dataset = dataset['val'].shuffle(seed=2020)
  test_dataset = dataset['test'].shuffle(seed=2020)

  training_args = TrainingArguments(output_dir = 'bert_base_finetuned_fake_bio_detector', per_gpu_train_batch_size = 16,\
                                    per_device_eval_batch_size=64, \
                                    do_eval = True, \
                                    num_train_epochs = 3, \
                                    report_to = 'wandb', \
                                    logging_strategy="steps", \
                                    evaluation_strategy="steps", \
                                    logging_steps = 300,\
                                    save_steps = 300,\
                                    load_best_model_at_end=True,\
                                    metric_for_best_model='accuracy')
                                    # push_to_hub=True)

  trainer = Trainer(
      model=model, args=training_args, train_dataset=train_dataset, eval_dataset=eval_dataset,\
      compute_metrics=compute_metrics
  )

  trainer.train()

  return trainer

"""## Model Evaluation on testset"""


def predictOnTestSet(trainer, dataset):
  preds = trainer.predict(dataset['test'])

  print("test accuracy", preds.metrics['test_accuracy'])

  return preds



"""## Load and run best model on Blind set"""

def tokenize_blind_data(example):
      return tokenizer(example['bio'], padding='max_length', truncation = True)


def getResultsOnBlindSet(model):
  dataset = load_dataset('csv',\
                         data_files={'blind_test': './data/processed_blind.test.txt'}, delimiter = '\n')

  

  dataset = dataset.map(tokenize_blind_data, batched=True)

  test_args = TrainingArguments(
      output_dir = './bert_base_finetuned_fake_bio_detector/checkpoint-1800',
      do_train = False,
      do_predict = True,
      per_device_eval_batch_size = 64,   
      dataloader_drop_last = False    
  )

  # init trainer
  trainer = Trainer(
                model = model, 
                args = test_args, 
                compute_metrics = compute_metrics)

  test_results = trainer.predict(dataset['blind_test'])
  test_labels = test_results.predictions.argmax(-1)
  return test_labels

"""## Write labels to CSV"""

def write__results_to_csv(filename):

  with open('./blind_test_results.csv', 'w') as f:
    writer = csv.DictWriter(f, fieldnames = {'label'})
    writer.writeheader()
    for l in test_labels:
      if l == 1:
        writer.writerow({'label' : '[FAKE]'})
      else:
        writer.writerow({'label' : '[REAL]'})


def plot_confusion_matrix(test_result):
  preds = test_result.predictions.argmax(-1)
  cm = confusion_matrix(preds, test_result.label_ids)
  plt.figure(figsize=(6, 6))
  sns.heatmap(cm, cmap="Blues", linecolor='black', linewidth=1, annot=True, fmt='', xticklabels=['REAL', 'FAKE'],
              yticklabels=['REAL', 'FAKE'])
  plt.xlabel("Predicted")
  plt.ylabel("Actual")
  plt.show()

  print(classification_report(test_result.label_ids, preds, target_names=['Predicted Fake', 'Predicted True']))

def main():

  # preprocess data
  add_labels_to_data()
  tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')
  dataset = get_dataloader(tokenizer)

  # define model
  model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased",  num_labels=2)

  # train model
  trainer = train(model, tokenizer, dataset)

  # test on test set
  preds = predictOnTestSet(trainer, dataset)

  # show results
  plot_confusion_matrix(preds)

  #### check results for best model #####
  

if __name__ == "__main__":
    main()
